import torch
import logging
import numpy as np
import torchvision.transforms.v2 as T
from torch.utils.data import DataLoader as TorchDataLoader
import sys
import os
current_file = os.path.abspath(__file__)
current_dir = os.path.dirname(current_file)
project_root = os.path.dirname(current_dir)
paths_to_add = [project_root, current_dir]
for path in paths_to_add:
    if path not in sys.path:
        sys.path.insert(0, path)
from wildtrack2_datasets import WildtrackDataset
from mot_datasets import MOTDataset
sys.path.insert(0, os.path.join(project_root, 'utils'))
from utils import collate_fn


def sample_augmentation(is_train, data_aug_conf):
    """
    é‡‡æ ·æ•°æ®å¢å¼ºå‚æ•°
    
    æ ¹æ®è®­ç»ƒ/éªŒè¯æ¨¡å¼ç”Ÿæˆä¸åŒçš„å›¾åƒå˜æ¢å‚æ•°ã€‚
    è®­ç»ƒæ—¶ä½¿ç”¨éšæœºç¼©æ”¾å’Œè£å‰ªï¼ŒéªŒè¯æ—¶ä½¿ç”¨å›ºå®šå˜æ¢ã€‚
    
    Args:
        is_train (bool): æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        data_aug_conf (dict): æ•°æ®å¢å¼ºé…ç½®å‚æ•°
    
    Returns:
        tuple: (resize_dims, crop)
            - resize_dims: ç¼©æ”¾åçš„å°ºå¯¸ (W, H)
            - crop: è£å‰ªå‚æ•° (x, y, x+W, y+H)
    """
    fH, fW = data_aug_conf['final_dim']
    if is_train:
        resize = np.random.uniform(*data_aug_conf['resize_lim'])
        resize_dims = (int(fW * resize), int(fH * resize))
        newW, newH = resize_dims

        # center it
        crop_h = int((newH - fH) / 2)
        crop_w = int((newW - fW) / 2)

        crop_offset = int(data_aug_conf['resize_lim'][0] * data_aug_conf['final_dim'][0])
        crop_w = crop_w + int(np.random.uniform(-crop_offset, crop_offset))
        crop_h = crop_h + int(np.random.uniform(-crop_offset, crop_offset))

        crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)
    else:  # validation/test
        # do a perfect resize
        resize_dims = (fW, fH)
        crop_h = 0
        crop_w = 0
        crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)
    return resize_dims, crop


def get_augmentation_transforms(train=True, data_aug_conf=None):
    """
    è·å–æ•°æ®å¢å¼ºå˜æ¢åˆ—è¡¨
    
    Args:
        train (bool): æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        data_aug_conf (dict): æ•°æ®å¢å¼ºé…ç½®å‚æ•°
    
    Returns:
        list: æ•°æ®å¢å¼ºå˜æ¢åˆ—è¡¨
    """
    transforms = []
    
    if train:
        if data_aug_conf is not None:
            resize_dims, crop = sample_augmentation(train, data_aug_conf)
            transforms.extend([
                T.Resize(size=resize_dims, antialias=True),
                T.RandomCrop(size=data_aug_conf['final_dim']),])
        
        transforms.extend([
            T.RandomShortestSize(min_size=600, max_size=1000, antialias=True),  # Base scale adjustment
            T.RandomHorizontalFlip(p=0.5),  # Random Horizontal Flip
            # T.RandomApply([T.RandomIoUCrop(min_scale=0.3, max_scale=1.0, min_aspect_ratio=0.5,
            #                                max_aspect_ratio=2.0, sampler_options={'min_iou': 0.3})], p=0.7),  # Random trimming
            T.RandomZoomOut(fill={0: (124, 116, 104)}, side_range=(1.0, 1.5), p=0.3)  # Random Expanded View
        ])
    else:
        # éªŒè¯/æµ‹è¯•æ¨¡å¼çš„å›ºå®šå˜æ¢
        if data_aug_conf is not None:
            resize_dims, crop = sample_augmentation(train, data_aug_conf)
            transforms.append(T.Resize(size=resize_dims, antialias=True))
    
    return transforms


def get_transform(train, data_aug_conf=None):
    """_summary_
    Ensure that horizontal flip is applied to the image, target and mask at the same time
    
    Args:
        train (bool): If True, apply data augmentation for training.
        data_aug_conf (dict): æ•°æ®å¢å¼ºé…ç½®å‚æ•°ï¼ŒåŒ…å« 'final_dim' å’Œ 'resize_lim'
    Returns:
        transforms (callable): A callable that applies the transformations.
    """
    # è·å–æ•°æ®å¢å¼ºå˜æ¢
    transforms = get_augmentation_transforms(train, data_aug_conf)
    
    # æ·»åŠ æ ‡å‡†åŒ–å’Œç±»å‹è½¬æ¢
    transforms.extend([
        T.ToDtype(torch.float, scale=True),
        T.Normalize(mean=[0.485, 0.456, 0.406], 
                   std=[0.229, 0.224, 0.225]),
        T.ToPureTensor()])
    
    return T.Compose(transforms)

class TransformSubset(torch.utils.data.Subset):
    def __init__(self, dataset, indices, transform=None):
        super().__init__(dataset, indices)
        self.transform = transform

    def __getitem__(self, idx):
        try:
            img, target, img_name, mask = self.dataset[self.indices[idx]]
            if self.transform is not None:
                img, target, mask = self.transform(img, target, mask)
            return img, target, img_name, mask
        except Exception as e:
            logging.error(f"Error loading image at index {idx}: {str(e)}")
            raise


class DataLoader:
    """
    é€šç”¨æ•°æ®åŠ è½½å™¨ç±»
    æ”¯æŒä»»æ„æ•°æ®é›†æ¨¡å—ï¼Œé»˜è®¤ä¸ºWildtrackDataset
    æä¾›è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½åŠŸèƒ½
    """
    def __init__(self, root_path, batch_size=4, num_workers=4, data_aug_conf=None, dataset_class=None,
                 **dataset_kwargs):
        """
        åˆå§‹åŒ–DataLoader
        
        Args:
            root_path (str): æ•°æ®é›†æ ¹è·¯å¾„
            batch_size (int): æ‰¹æ¬¡å¤§å°
            num_workers (int): å·¥ä½œè¿›ç¨‹æ•°
            data_aug_conf (dict): æ•°æ®å¢å¼ºé…ç½®
            dataset_class (class): æ•°æ®é›†ç±»ï¼Œé»˜è®¤ä¸ºWildtrackDataset
            **dataset_kwargs: ä¼ é€’ç»™æ•°æ®é›†ç±»çš„é¢å¤–å‚æ•°
        """
        if dataset_class is None:
            dataset_class = WildtrackDataset
            
        self.dataset_class = dataset_class
        self.root_path = root_path
        self.dataset_kwargs = dataset_kwargs
        self.batch_size = batch_size
        self.num_workers = num_workers
        
        if data_aug_conf is None: # è®¾ç½®é»˜è®¤æ•°æ®å¢å¼ºé…ç½®
            self.data_aug_conf = {
                'final_dim': [800, 600],  # æœ€ç»ˆè¾“å‡ºå°ºå¯¸ (W, H)
                'resize_lim': [0.8, 1.2]}  # ç¼©æ”¾èŒƒå›´
        else:
            self.data_aug_conf = data_aug_conf
            
        self._initialize_datasets()  # åˆå§‹åŒ–æ•°æ®é›†
        self._create_splits()        # åˆ›å»ºæ•°æ®é›†åˆ†å‰²
        self._create_dataloaders()   # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    
    def _initialize_datasets(self):
        """æ ¹æ®æ•°æ®é›†ç±»å‹åˆå§‹åŒ–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†"""
        if self.dataset_class.__name__ == 'MOTDataset':
            # å¯¹äºMOTDatasetï¼Œåˆ†åˆ«åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
            datasets = self.dataset_kwargs.get('datasets', ['MOT16'])
            
            # åˆ›å»ºè®­ç»ƒæ•°æ®é›† (ç”¨äºè®­ç»ƒå’ŒéªŒè¯)
            self.train_dataset_raw = self.dataset_class(
                root=self.root_path, 
                datasets=datasets, 
                split='train', 
                transforms=None,
                sample_ratio=1.0,  # ç¦ç”¨å†…éƒ¨é‡‡æ ·
                max_samples=None   # ç¦ç”¨æ ·æœ¬é™åˆ¶
            )
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®é›† (ç”¨äºéªŒè¯å’Œæµ‹è¯•)
            self.test_dataset_raw = self.dataset_class(
                root=self.root_path, 
                datasets=datasets, 
                split='test', 
                transforms=None,
                sample_ratio=1.0,  # ç¦ç”¨å†…éƒ¨é‡‡æ ·
                max_samples=None   # ç¦ç”¨æ ·æœ¬é™åˆ¶
            )
            logging.info(f"MOTDataset initialized:")
            logging.info(f"  - Train split size: {len(self.train_dataset_raw)}")
            logging.info(f"  - Test split size: {len(self.test_dataset_raw)}")
        else:
            # å¯¹äºå…¶ä»–æ•°æ®é›†ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
            self.dataset = self.dataset_class(root=self.root_path, transforms=None)
            logging.info(f"{self.dataset_class.__name__} initialized with {len(self.dataset)} samples")
    
    def _validate_data_sources(self):
        """
        éªŒè¯æ•°æ®æ¥æºçš„æ­£ç¡®æ€§
        """
        if self.dataset_class.__name__ == 'MOTDataset':
            # éªŒè¯æ•°æ®æ¥æº
            checks = {
                'è®­ç»ƒé›†': self.train_dataset.dataset is self.train_dataset_raw,
                'éªŒè¯é›†': self.val_dataset.dataset is self.train_dataset_raw,
                'æµ‹è¯•é›†': self.test_dataset.dataset is self.test_dataset_raw
            }
            
            print(f"\nğŸ” æ•°æ®æ¥æºéªŒè¯:")
            for name, is_correct in checks.items():
                status = "âœ… æ­£ç¡®" if is_correct else "âŒ é”™è¯¯"
                print(f"   - {name}: {status}")
            
            if not all(checks.values()):
                logging.warning("âš ï¸ æ•°æ®æ¥æºéªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥åˆ†å‰²é€»è¾‘ã€‚")
    
    def _create_splits(self):
        """åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†åˆ†å‰²"""
        if self.dataset_class.__name__ == 'MOTDataset':
            # ğŸ¯ MOTDatasetçš„æ–°åˆ†å‰²é€»è¾‘
            train_total_size = len(self.train_dataset_raw)
            test_total_size = len(self.test_dataset_raw)
            
            # ğŸ“Š æ–°çš„åˆ†å‰²ç­–ç•¥
            # 1. ä»è®­ç»ƒé›†é‡‡æ ·50%
            train_sample_ratio = self.dataset_kwargs.get('train_sample_ratio', 0.5)
            train_sample_size = int(train_total_size * train_sample_ratio)
            
            # 2. ä»é‡‡æ ·çš„è®­ç»ƒé›†ä¸­åˆ†å‡º10%ä½œä¸ºéªŒè¯é›†
            val_ratio_from_train = self.dataset_kwargs.get('val_ratio_from_train', 0.1)
            val_size = int(train_sample_size * val_ratio_from_train)
            # actual_train_size = train_sample_size - val_size
            
            # 3. æµ‹è¯•é›†æ•°é‡ä¸éªŒè¯é›†ç›¸åŒ
            test_size = val_size
            
            # ğŸ² ç”Ÿæˆéšæœºç´¢å¼•
            # è®­ç»ƒé›†ç´¢å¼•ï¼ˆç”¨äºè®­ç»ƒ+éªŒè¯ï¼‰
            train_all_indices = torch.randperm(train_total_size)[:train_sample_size].tolist()
            
            # ä»è®­ç»ƒé›†ç´¢å¼•ä¸­åˆ†å‰²å‡ºéªŒè¯é›†
            val_indices = train_all_indices[:val_size]
            actual_train_indices = train_all_indices[val_size:]
            
            # æµ‹è¯•é›†ç´¢å¼•ï¼ˆä»åŸå§‹æµ‹è¯•é›†ä¸­é‡‡æ ·ï¼‰
            test_indices = torch.randperm(test_total_size)[:test_size].tolist()
            
            # ğŸ—ï¸ åˆ›å»ºæ•°æ®é›†å­é›†
            self.train_dataset = TransformSubset(self.train_dataset_raw, actual_train_indices)
            self.val_dataset = TransformSubset(self.train_dataset_raw, val_indices)  # âœ… æ¥è‡ªè®­ç»ƒé›†
            self.test_dataset = TransformSubset(self.test_dataset_raw, test_indices)
            
            # ğŸ†• å­˜å‚¨ç´¢å¼•ä¿¡æ¯ç”¨äºç»Ÿè®¡
            self._train_indices = actual_train_indices
            self._val_indices = val_indices
            self._test_indices = test_indices
            
            # ğŸ“ˆ æ‰“å°åˆ†å‰²ä¿¡æ¯
            logging.info(f"MOTæ•°æ®é›†åˆ†å‰²å®Œæˆ:")
            logging.info(f"  - åŸå§‹è®­ç»ƒé›†: {train_total_size}")
            logging.info(f"  - åŸå§‹æµ‹è¯•é›†: {test_total_size}")
            logging.info(f"  - è®­ç»ƒé›†é‡‡æ ·: {train_sample_size}/{train_total_size} ({train_sample_ratio:.1%})")
            logging.info(f"  - æœ€ç»ˆè®­ç»ƒé›†: {len(actual_train_indices)} (æ¥è‡ªè®­ç»ƒé›†)")
            logging.info(f"  - éªŒè¯é›†: {len(val_indices)} (æ¥è‡ªè®­ç»ƒé›†, {val_ratio_from_train:.1%})")
            logging.info(f"  - æµ‹è¯•é›†: {len(test_indices)} (æ¥è‡ªæµ‹è¯•é›†, ä¸éªŒè¯é›†æ•°é‡ç›¸åŒ)")
            
            # ğŸ” éªŒè¯æ•°æ®æ¥æº
            self._validate_data_sources()
            
        else:
            # å…¶ä»–æ•°æ®é›†çš„åˆ†å‰²é€»è¾‘ä¿æŒä¸å˜
            total_size = len(self.dataset)
            train_size = int(0.7 * total_size)
            val_size = int(0.15 * total_size)
            test_size = total_size - train_size - val_size
            
            indices = torch.randperm(total_size)
            train_indices = indices[:train_size].tolist()
            val_indices = indices[train_size:train_size + val_size].tolist()
            test_indices = indices[train_size + val_size:].tolist()
            
            self.train_dataset = TransformSubset(self.dataset, train_indices)
            self.val_dataset = TransformSubset(self.dataset, val_indices)
            self.test_dataset = TransformSubset(self.dataset, test_indices)
            
            self._train_indices = train_indices
            self._val_indices = val_indices
            self._test_indices = test_indices
            
            logging.info(f"æ•°æ®é›†åˆ†å‰²: è®­ç»ƒé›†{train_size}, éªŒè¯é›†{val_size}, æµ‹è¯•é›†{test_size}")
    
    def _create_dataloaders(self):
        """Create data loaders for train, validation, and test datasets"""
        self.train_loader = TorchDataLoader(
            self.train_dataset, 
            batch_size=self.batch_size, 
            shuffle=True,
            collate_fn=collate_fn, 
            num_workers=self.num_workers,
            pin_memory=True, 
            persistent_workers=True if self.num_workers > 0 else False)
        
        self.val_loader = TorchDataLoader(
            self.val_dataset, 
            batch_size=self.batch_size, 
            shuffle=False,
            collate_fn=collate_fn, 
            num_workers=self.num_workers,
            pin_memory=True, 
            persistent_workers=True if self.num_workers > 0 else False)
        
        self.test_loader = TorchDataLoader(
            self.test_dataset, 
            batch_size=self.batch_size, 
            shuffle=False,
            collate_fn=collate_fn, 
            num_workers=self.num_workers,
            pin_memory=True, 
            persistent_workers=True if self.num_workers > 0 else False)
    
    def get_dataloaders(self):
        """
        è·å–è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
        
        Returns:
            tuple: (train_loader, val_loader, test_loader)
        """
        return self.train_loader, self.val_loader, self.test_loader
    
    def get_dataset_info(self):
        """
        è·å–æ•°æ®é›†ä¿¡æ¯
        å¢åŠ æ•°æ®æ¥æºä¿¡æ¯
        """
        info = {
            'dataset_class': self.dataset_class.__name__,
            'root_path': self.root_path,
            'batch_size': self.batch_size,
            'num_workers': self.num_workers,
            'data_aug_conf': self.data_aug_conf
        }
        
        if self.dataset_class.__name__ == 'MOTDataset':
            # è®¡ç®—æ¯”ä¾‹
            train_raw_size = len(self.train_dataset_raw)
            test_raw_size = len(self.test_dataset_raw)
            
            train_size = len(self.train_dataset.indices) if hasattr(self.train_dataset, 'indices') else 0
            val_size = len(self.val_dataset.indices) if hasattr(self.val_dataset, 'indices') else 0
            test_size = len(self.test_dataset.indices) if hasattr(self.test_dataset, 'indices') else 0
            
            # MOTDataset ç›¸å…³ä¿¡æ¯
            info.update({
                'train_raw_size': train_raw_size,
                'test_raw_size': test_raw_size,
                'train_size': train_size,
                'val_size': val_size,
                'test_size': test_size,
                'train_sample_ratio': self.dataset_kwargs.get('train_sample_ratio', 0.5),
                'val_ratio_from_train': self.dataset_kwargs.get('val_ratio_from_train', 0.1),
                # ğŸ†• æ•°æ®æ¥æºä¿¡æ¯
                'train_source': 'train_dataset_raw',
                'val_source': 'train_dataset_raw (ä»è®­ç»ƒé›†åˆ†å‰²)',
                'test_source': 'test_dataset_raw',
                # è®¡ç®—å®é™…æ¯”ä¾‹
                'val_ratio_of_total_train': val_size / train_raw_size if train_raw_size > 0 else 0,
                'test_ratio_of_total_test': test_size / test_raw_size if test_raw_size > 0 else 0
            })
        else:
            # å…¶ä»–æ•°æ®é›†ä¿¡æ¯
            if hasattr(self, 'dataset'):
                total_size = len(self.dataset)
                info.update({
                    'total_size': total_size,
                    'train_size': len(self.train_dataset.indices) if hasattr(self.train_dataset, 'indices') else 0,
                    'val_size': len(self.val_dataset.indices) if hasattr(self.val_dataset, 'indices') else 0,
                    'test_size': len(self.test_dataset.indices) if hasattr(self.test_dataset, 'indices') else 0
                })
        
        return info
    
    def print_dataset_info(self):
        """
        æ‰“å°æ•°æ®é›†ä¿¡æ¯
        å¢å¼ºæ•°æ®æ¥æºæ˜¾ç¤º
        """
        try:
            info = self.get_dataset_info()
            
            print(f"\nğŸ“Š {info['dataset_class']} DataLoader Information:")
            print(f"   - æ•°æ®é›†ç±»å‹: {info['dataset_class']}")
            print(f"   - æ ¹è·¯å¾„: {info['root_path']}")
            print(f"   - æ‰¹æ¬¡å¤§å°: {info['batch_size']}")
            print(f"   - å·¥ä½œè¿›ç¨‹: {info['num_workers']}")
            
            if info['dataset_class'] == 'MOTDataset':
                print(f"\nğŸ“ˆ MOTæ•°æ®é›†ç»Ÿè®¡:")
                print(f"   - åŸå§‹è®­ç»ƒé›†: {info.get('train_raw_size', 'N/A')}")
                print(f"   - åŸå§‹æµ‹è¯•é›†: {info.get('test_raw_size', 'N/A')}")
                print(f"   - æœ€ç»ˆè®­ç»ƒé›†: {info.get('train_size', 'N/A')} ({info.get('train_source', 'N/A')})")
                print(f"   - éªŒè¯é›†: {info.get('val_size', 'N/A')} ({info.get('val_source', 'N/A')})")
                print(f"   - æµ‹è¯•é›†: {info.get('test_size', 'N/A')} ({info.get('test_source', 'N/A')})")
                
                print(f"\nğŸ“Š é‡‡æ ·æ¯”ä¾‹:")
                print(f"   - è®­ç»ƒé›†é‡‡æ ·æ¯”ä¾‹: {info.get('train_sample_ratio', 'N/A'):.1%}")
                print(f"   - éªŒè¯é›†å è®­ç»ƒé›†æ¯”ä¾‹: {info.get('val_ratio_from_train', 'N/A'):.1%}")
                print(f"   - éªŒè¯é›†å æ€»è®­ç»ƒé›†æ¯”ä¾‹: {info.get('val_ratio_of_total_train', 0):.1%}")
                print(f"   - æµ‹è¯•é›†å æ€»æµ‹è¯•é›†æ¯”ä¾‹: {info.get('test_ratio_of_total_test', 0):.1%}")
            else:
                print(f"\nğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡:")
                print(f"   - æ€»å¤§å°: {info.get('total_size', 'N/A')}")
                print(f"   - è®­ç»ƒé›†: {info.get('train_size', 'N/A')}")
                print(f"   - éªŒè¯é›†: {info.get('val_size', 'N/A')}")
                print(f"   - æµ‹è¯•é›†: {info.get('test_size', 'N/A')}")
            
            print(f"\nğŸ”§ æ•°æ®å¢å¼ºé…ç½®:")
            print(f"   - æœ€ç»ˆå°ºå¯¸: {info['data_aug_conf']['final_dim']}")
            print(f"   - ç¼©æ”¾èŒƒå›´: {info['data_aug_conf']['resize_lim']}")
            
        except Exception as e:
            print(f"âŒ æ‰“å°æ•°æ®é›†ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            print("ğŸ” å¯ç”¨å±æ€§:", [attr for attr in dir(self) if not attr.startswith('_')])

def create_wildtrack_dataloaders(root_path, batch_size=4, num_workers=4, data_aug_conf=None, dataset_class=None):
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå‘åå…¼å®¹å‡½æ•°ï¼‰
    
    Args:
        root_path (str): æ•°æ®é›†æ ¹è·¯å¾„
        batch_size (int): æ‰¹æ¬¡å¤§å°
        num_workers (int): å·¥ä½œè¿›ç¨‹æ•°
        data_aug_conf (dict): æ•°æ®å¢å¼ºé…ç½®
        dataset_class (class): æ•°æ®é›†ç±»ï¼Œé»˜è®¤ä¸ºWildtrackDataset
    
    Returns:
        tuple: (train_loader, val_loader, test_loader)
    """
    dataloader = DataLoader(root_path, batch_size, num_workers, data_aug_conf, dataset_class)
    return dataloader.get_dataloaders()

if __name__ == "__main__":
    def wildtrack2_dataloader_test():
        root_path = "/home/s-jiang/Documents/datasets/Wildtrack2"
        try:
            # Methode 1ï¼šuse default dataset: WildtrackDataset
            dataloader = DataLoader(root_path, batch_size=2, num_workers=2)
            dataloader.print_dataset_info()
            # Methode 2ï¼šUse custom dataset class
            # from some_other_dataset import SomeDataset
            # dataloader = DataLoader(root_path, batch_size=2, num_workers=2, dataset_class=SomeDataset)
            train_loader, val_loader, test_loader = dataloader.get_dataloaders()
            # Test data loading
            print("\n=== Testing data loading ===")
            for batch_idx, (images, targets, img_names, masks) in enumerate(train_loader):
                print(f"Batch {batch_idx}: {len(images)} images loaded")
                print(f"Image shapes: {[img.shape for img in images]}")
                print(f"Image names: {img_names}")
                if batch_idx == 0:
                    break     
        except Exception as e:
            print(f"Error: {e}")
            print("Please check the dataset path and configuration.")
            import traceback
            traceback.print_exc()
    def mot_dataloader_test():
        root_path = "/home/s-jiang/Documents/datasets"
        try:
            print("=== Testing MOTDataset ===")
            mot_dataloader = DataLoader(
                root_path=root_path,
                batch_size=8, 
                num_workers=8,
                dataset_class=MOTDataset,
                datasets=['MOT16', 'MOT17', 'MOT20'],  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ•°æ®é›†
                split='train'  # è¿™ä¸ªå‚æ•°åœ¨æ–°é€»è¾‘ä¸­ä¸ä¼šå½±å“åˆ†å‰²ç­–ç•¥
            )
            mot_dataloader.print_dataset_info()
            
            train_loader, val_loader, test_loader = mot_dataloader.get_dataloaders()
            
            # Test data loading
            print("\n=== Testing MOT data loading ===")
            for batch_idx, (images, targets, img_names, masks) in enumerate(train_loader):
                print(f"Train Batch {batch_idx}: {len(images)} images loaded")
                if batch_idx == 0:
                    break
                    
            for batch_idx, (images, targets, img_names, masks) in enumerate(val_loader):
                print(f"Val Batch {batch_idx}: {len(images)} images loaded")
                if batch_idx == 0:
                    break
                    
        except Exception as e:
            print(f"Error testing MOTDataset: {e}")
            import traceback
            traceback.print_exc()
        
    # wildtrack2_dataloader_test()
    mot_dataloader_test()